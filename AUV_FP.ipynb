{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AUV_FP",
      "provenance": [],
      "mount_file_id": "1sS-5TnDkVUtygIy6geduasV8PTAFpT8f",
      "authorship_tag": "ABX9TyN1aYjgSi+jTB8exY+ra/RY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akstud/Ocean-object_classification/blob/master/AUV_FP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp4_YRe4uoa5",
        "colab_type": "text"
      },
      "source": [
        "***IMPORTING SOME BASIC LIBRARIES***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t6EWhec5eRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBB8bAnguxcI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "***UPLOADING MY DATASET***\n",
        "\n",
        "It is uploaded in rar format and has been unrared"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWq-rj16RK2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trying to unrar my folder\n",
        "!unrar x \"/content/drive/My Drive/dataset_AUV/train.rar\" \"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc4QzViWyaT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unrar x \"/content/drive/My Drive/dataset_AUV/test_col.rar\" \"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-wtVfa5wx5I",
        "colab_type": "text"
      },
      "source": [
        "***CLASSES THAT I HAVE USED***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MufWRA93K9vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dat=[\"coral reef\",\"diver\",\"fish\",\"jelly fish\",\"plant\",\"robot\",\"ruins\",\"shark\",\"starfish\",\"turtle\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI-_EqUZw-c0",
        "colab_type": "text"
      },
      "source": [
        "***CALLBACKS***\n",
        "\n",
        "To stop the training at 99% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K22U6Rayu9jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if (logs.get('acc') is not None and logs.get('acc') > 0.99):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        " \n",
        "callbacks = myCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WriwseDZxT36",
        "colab_type": "text"
      },
      "source": [
        "**MY MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txRxA9CLSJ_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=( 150,150, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "   \n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        \n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        \n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        \n",
        "\n",
        "        #tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        #tf.keras.layers.MaxPooling2D(2,2),\n",
        "        \n",
        "        tf.keras.layers.Flatten(),\n",
        "        \n",
        "        tf.keras.layers.Dense(128,activation = 'relu'),\n",
        "        \n",
        "        tf.keras.layers.Dense(10,activation = 'softmax')\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4WhS6HVveZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "6302bb3b-0d5f-4a89-fbe6-cbf8b166402e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 148, 148, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 74, 74, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 70, 70, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 68, 68, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 34, 34, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 509,514\n",
            "Trainable params: 509,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uef0ZvgxpCW",
        "colab_type": "text"
      },
      "source": [
        "***CONFIGURATIONS***\n",
        "with some chosen hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWy7Rt6Tvurn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = RMSprop(lr = 0.001),\n",
        "                  metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOgiURrJxvcg",
        "colab_type": "text"
      },
      "source": [
        "***DATA PREPROCESSING***\n",
        "\n",
        "my training images are labelled, shuffled, resized in this function\n",
        "\n",
        "batch size of 64 chosen\n",
        "\n",
        "and class mode is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBXRUQs-vnA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d25c241e-2e22-474a-fc94-9bdc44722c78"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/train/',\n",
        "                                                        target_size = (150,150),\n",
        "                                                        shuffle = True,\n",
        "                                                        batch_size = 64,\n",
        "                                                        class_mode = 'categorical')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4109 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-s5vA9ix3A5",
        "colab_type": "text"
      },
      "source": [
        "***TRAINING MY MODEL***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LE7ewkzv7YB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3aa6f08-5296-447b-f259-35072d82bba7"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = 8,\n",
        "                              epochs = 100,\n",
        "                              verbose = 1,\n",
        "                              callbacks = [callbacks])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 3s 367ms/step - loss: 0.7066 - acc: 0.8418\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 3s 371ms/step - loss: 0.1241 - acc: 0.9590\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 3s 313ms/step - loss: 0.1206 - acc: 0.9629\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 3s 362ms/step - loss: 0.2361 - acc: 0.9284\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 3s 398ms/step - loss: 0.1207 - acc: 0.9629\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 2s 310ms/step - loss: 0.1569 - acc: 0.9566\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 2s 307ms/step - loss: 0.1692 - acc: 0.9395\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 3s 317ms/step - loss: 0.0825 - acc: 0.9707\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 3s 352ms/step - loss: 0.1995 - acc: 0.9395\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 3s 356ms/step - loss: 0.2392 - acc: 0.9297\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 3s 431ms/step - loss: 0.1816 - acc: 0.9434\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 3s 435ms/step - loss: 0.1614 - acc: 0.9453\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 4s 478ms/step - loss: 0.3320 - acc: 0.9141\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 3s 343ms/step - loss: 0.0607 - acc: 0.9785\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 3s 362ms/step - loss: 0.1698 - acc: 0.9434\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 3s 316ms/step - loss: 0.0585 - acc: 0.9863\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 3s 399ms/step - loss: 0.2444 - acc: 0.9297\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 3s 404ms/step - loss: 0.1174 - acc: 0.9648\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 3s 313ms/step - loss: 0.0665 - acc: 0.9783\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 3s 342ms/step - loss: 0.2289 - acc: 0.9277\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 2s 290ms/step - loss: 0.2111 - acc: 0.9434\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 3s 343ms/step - loss: 0.2002 - acc: 0.9453\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 4s 518ms/step - loss: 0.0665 - acc: 0.9824\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 3s 400ms/step - loss: 0.0573 - acc: 0.9844\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 3s 357ms/step - loss: 0.2256 - acc: 0.9258\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 3s 409ms/step - loss: 0.0867 - acc: 0.9766\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.0783 - acc: 0.9727\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 2s 310ms/step - loss: 0.0523 - acc: 0.9883\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 3s 425ms/step - loss: 0.3642 - acc: 0.9238\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 3s 386ms/step - loss: 0.0762 - acc: 0.9785\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 3s 315ms/step - loss: 0.0914 - acc: 0.9746\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 2s 309ms/step - loss: 0.0696 - acc: 0.9805\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 4s 451ms/step - loss: 0.1919 - acc: 0.9395\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 0.1042 - acc: 0.9675\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 3s 329ms/step - loss: 0.2910 - acc: 0.9414\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 3s 363ms/step - loss: 0.1639 - acc: 0.9610\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 2s 289ms/step - loss: 0.0997 - acc: 0.9675\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 3s 381ms/step - loss: 0.0692 - acc: 0.9805\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 3s 428ms/step - loss: 0.2020 - acc: 0.9436\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 3s 353ms/step - loss: 0.1063 - acc: 0.9785\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 3s 351ms/step - loss: 0.0492 - acc: 0.9805\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 3s 327ms/step - loss: 0.3963 - acc: 0.9023\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0329 - acc: 0.9902\n",
            "Reached 99% accuracy so cancelling training!\n",
            "8/8 [==============================] - 4s 466ms/step - loss: 0.0329 - acc: 0.9902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc00AchP9Ebl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "NOW I ALSO NEED TO ADD DATA AUGMENTATION TO INCREASE THE NUMBER OF IMAGES IN MY DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UcsZ0_170lz",
        "colab_type": "text"
      },
      "source": [
        "***TESTING***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nCdcinYsxdW",
        "colab_type": "text"
      },
      "source": [
        "#*** testing the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKkTboGZjXt_",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0264647d-baab-48f2-c3e2-97264d288fcc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded=files.upload()\n",
        "a=0\n",
        "c=0\n",
        "for fn in uploaded.keys():\n",
        "  c=c+1 \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=image.load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  x=image.img_to_array(img)\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  if classes[0][0]==1:\n",
        "    print(\"coral reef\")    \n",
        "  elif classes[0][1]==1:\n",
        "    print(\"divers\")\n",
        "    \n",
        "  elif classes[0][2]==1:\n",
        "    print(\"fish\")\n",
        "    \n",
        "  elif classes[0][3]==1:\n",
        "    print(\"jellyfish\")\n",
        "    \n",
        "  elif classes[0][4]==1:\n",
        "    print(\"plant\")\n",
        "    \n",
        "  elif classes[0][5]==1:\n",
        "    print(\"robot\")\n",
        "    \n",
        "  elif classes[0][6]==1:\n",
        "    print(\"ruins\")\n",
        "    \n",
        "  elif classes[0][7]==1:\n",
        "    print(\"shark\")\n",
        "   \n",
        "  elif classes[0][8]==1:\n",
        "    print(\"starfish\")\n",
        "    a=a+1\n",
        "  else:\n",
        "    print(\"turtle\")\n",
        "    a=a+1\n",
        "  \n",
        "print(a*100/c)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-083f3539-b5f3-4bf7-9f65-e8d28991e6d8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-083f3539-b5f3-4bf7-9f65-e8d28991e6d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving turtle (1).jpeg to turtle (1).jpeg\n",
            "Saving turtle (1).jpg to turtle (1).jpg\n",
            "Saving turtle (1).png to turtle (1).png\n",
            "Saving turtle (2).jpeg to turtle (2).jpeg\n",
            "Saving turtle (2).jpg to turtle (2).jpg\n",
            "Saving turtle (2).png to turtle (2).png\n",
            "Saving turtle (3).jpg to turtle (3).jpg\n",
            "Saving turtle (4).jpg to turtle (4).jpg\n",
            "Saving turtle (5).jpg to turtle (5).jpg\n",
            "Saving turtle (6).jpg to turtle (6).jpg\n",
            "Saving turtle (7).jpg to turtle (7).jpg\n",
            "Saving turtle (8).jpg to turtle (8).jpg\n",
            "Saving turtle (9).jpg to turtle (9).jpg\n",
            "Saving turtle (10).jpg to turtle (10).jpg\n",
            "Saving turtle (11).jpg to turtle (11).jpg\n",
            "Saving turtle (12).jpg to turtle (12).jpg\n",
            "Saving turtle (14).jpg to turtle (14).jpg\n",
            "Saving turtle (15).jpg to turtle (15).jpg\n",
            "Saving turtle (16).jpg to turtle (16).jpg\n",
            "Saving turtle (17).jpg to turtle (17).jpg\n",
            "Saving turtle (18).jpg to turtle (18).jpg\n",
            "Saving turtle (19).jpg to turtle (19).jpg\n",
            "Saving turtle (20).jpg to turtle (20).jpg\n",
            "Saving turtle (21).jpg to turtle (21).jpg\n",
            "Saving turtle (22).jpg to turtle (22).jpg\n",
            "Saving turtle (24).jpg to turtle (24).jpg\n",
            "Saving turtle (25).jpg to turtle (25).jpg\n",
            "Saving turtle (26).jpg to turtle (26).jpg\n",
            "Saving turtle (27).jpg to turtle (27).jpg\n",
            "Saving turtle (28).jpg to turtle (28).jpg\n",
            "Saving turtle (29).jpg to turtle (29).jpg\n",
            "Saving turtle (30).jpg to turtle (30).jpg\n",
            "Saving turtle (31).jpg to turtle (31).jpg\n",
            "Saving turtle (32).jpg to turtle (32).jpg\n",
            "Saving turtle (33).jpg to turtle (33).jpg\n",
            "Saving turtle (34).jpg to turtle (34).jpg\n",
            "Saving turtle (35).jpg to turtle (35).jpg\n",
            "Saving turtle (36).jpg to turtle (36).jpg\n",
            "Saving turtle (37).jpg to turtle (37).jpg\n",
            "Saving turtle (38).jpg to turtle (38).jpg\n",
            "Saving turtle (39).jpg to turtle (39).jpg\n",
            "Saving turtle (40).jpg to turtle (40).jpg\n",
            "Saving turtle (41).jpg to turtle (41).jpg\n",
            "Saving turtle (42).jpg to turtle (42).jpg\n",
            "Saving turtle (43).jpg to turtle (43).jpg\n",
            "Saving turtle (44).jpg to turtle (44).jpg\n",
            "Saving turtle (45).jpg to turtle (45).jpg\n",
            "Saving turtle (46).jpg to turtle (46).jpg\n",
            "Saving turtle (47).jpg to turtle (47).jpg\n",
            "Saving turtle (48).jpg to turtle (48).jpg\n",
            "Saving turtle (49).jpg to turtle (49).jpg\n",
            "Saving turtle (50).jpg to turtle (50).jpg\n",
            "Saving turtle (51).jpg to turtle (51).jpg\n",
            "Saving turtle (52).jpg to turtle (52).jpg\n",
            "Saving turtle (53).jpg to turtle (53).jpg\n",
            "Saving turtle (54).jpg to turtle (54).jpg\n",
            "Saving turtle (55).jpg to turtle (55).jpg\n",
            "Saving turtle (56).jpg to turtle (56).jpg\n",
            "Saving turtle (57).jpg to turtle (57).jpg\n",
            "Saving turtle (58).jpg to turtle (58).jpg\n",
            "Saving turtle (59).jpg to turtle (59).jpg\n",
            "Saving turtle (60).jpg to turtle (60).jpg\n",
            "Saving turtle (323).jpg to turtle (323).jpg\n",
            "Saving turtle (324).jpg to turtle (324).jpg\n",
            "Saving turtle (325).jpg to turtle (325).jpg\n",
            "Saving turtle (326).jpg to turtle (326).jpg\n",
            "Saving turtle (327).jpg to turtle (327).jpg\n",
            "Saving turtle (328).jpg to turtle (328).jpg\n",
            "Saving turtle (330).jpg to turtle (330).jpg\n",
            "Saving turtle (331).jpg to turtle (331).jpg\n",
            "Saving turtle (332).jpg to turtle (332).jpg\n",
            "Saving turtle.jpg to turtle.jpg\n",
            "turtle\n",
            "turtle\n",
            "starfish\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "fish\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "starfish\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "turtle\n",
            "98.61111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s31s4OF_A8A",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#test accuracy for different objects\n",
        "coral reef- 72%\n",
        "\n",
        "shark- 48%\n",
        "\n",
        "diver - 50%\n",
        "\n",
        "fish - 79.4%\n",
        "\n",
        "jellyfish- 88.09\n",
        "\n",
        "plant-91%\n",
        "\n",
        "robot- 84.84%\n",
        "\n",
        "ruins-41.37%\n",
        "\n",
        "starfish - 97.9\n",
        "\n",
        "turtle- 98.6%\n",
        "\n",
        "overall accuracy=75.12\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqSsvosOYoZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dictionary for rgb values\n",
        "\n",
        "color_code = {\n",
        "    'light blue':[64,156,174],\n",
        "    'blue':[28,118,150],\n",
        "    'dark blue': [27,56,90],\n",
        "    'green': [95,136,72]\n",
        "\n",
        "}"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ipnJf8tIIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TO FIND THE EUCLIDIAN DISTANCE BETWEEN THE NEAREST COLOUR\n",
        "\n",
        "import numpy as np\n",
        "def dist(cord):#coordinates\n",
        "  min=70000000\n",
        "  index=''\n",
        "  for key,val in color_code.items():\n",
        "    distance=np.linalg.norm(val-cord)\n",
        "    if distance<min:\n",
        "      min=distance\n",
        "      index=key\n",
        "\n",
        "  return index\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmHQoGqv6ckE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TO FIND THE ACCURACY OF COLOUR DETECTION\n",
        "\n",
        "def acc():\n",
        "  path_blue='drive/My Drive/Colab Notebooks/test_col/blue/'\n",
        "  path_darkb='drive/My Drive/Colab Notebooks/test_col/dark blue/'\n",
        "  path_lightb='drive/My Drive/Colab Notebooks/test_col/light blue/'\n",
        "  path_green='drive/My Drive/Colab Notebooks/test_col/green/'\n",
        "  tot=0 #total\n",
        "  cor=0 #correct\n",
        "  green=glob.glob(path_green+'*')\n",
        "  blue= glob.glob(path_blue+'*')\n",
        "  darkb= glob.glob(path_darkb+'*')\n",
        "  lightb= glob.glob(path_lightb+'*')\n",
        "\n",
        "  for i in range(len(green)):\n",
        "    tot=tot+1\n",
        "    dc = DominantColors(green[i], clusters) \n",
        "    colors = dc.dominantColors()  \n",
        "    if dist(colors)=='green':\n",
        "      cor=cor+1\n",
        "  \n",
        "  for i in range(len(blue)):\n",
        "    tot= tot + 1\n",
        "    dc = DominantColors(blue[i], clusters) \n",
        "    colors = dc.dominantColors()  \n",
        "    #print(dist(colors))\n",
        "    #print(blue[i])\n",
        "    if dist(colors)=='blue':\n",
        "      cor=cor +1\n",
        "    \n",
        "  for i in range(len(darkb)):\n",
        "    tot=tot + 1\n",
        "    dc = DominantColors(darkb[i], clusters) \n",
        "    colors = dc.dominantColors()  \n",
        "    #print(dist(colors))\n",
        "    if dist(colors)=='dark blue':\n",
        "      cor=cor + 1\n",
        "  \n",
        "  for i in range(len(lightb)):\n",
        "    tot= tot + 1\n",
        "    dc = DominantColors(lightb[i], clusters) \n",
        "    colors = dc.dominantColors()  \n",
        "    #print(dist(colors))\n",
        "    if dist(colors)=='light blue':\n",
        "      cor= cor + 1\n",
        "  print(cor*100/tot)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTyf2uSBml06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a4a550bc-7349-4550-f69b-4a43655faeb5"
      },
      "source": [
        "#USING KMEANS TO DETECT COLOUR\n",
        "\n",
        "import cv2\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.preprocessing import image\n",
        "import glob\n",
        "\n",
        "class DominantColors:\n",
        "\n",
        "    CLUSTERS = None\n",
        "    IMAGE = None\n",
        "    COLORS = None\n",
        "    LABELS = None\n",
        "    \n",
        "    \n",
        "    def __init__(self, image, clusters=3):\n",
        "        self.CLUSTERS = clusters\n",
        "        self.IMAGE = image\n",
        "        \n",
        "    def dominantColors(self):\n",
        "    \n",
        "        #read image\n",
        "        img = cv2.imread(self.IMAGE)\n",
        "        \n",
        "        #convert to rgb from bgr\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "        #reshaping to a list of pixels\n",
        "        img = img.reshape((img.shape[0] * img.shape[1], 3))\n",
        "        \n",
        "        #save image after operations\n",
        "        self.IMAGE = img\n",
        "        \n",
        "        #using k-means to cluster pixels\n",
        "        kmeans = KMeans(n_clusters = self.CLUSTERS)\n",
        "        kmeans.fit(img)\n",
        "        \n",
        "        #the cluster centers are our dominant colors.\n",
        "        self.COLORS = kmeans.cluster_centers_\n",
        "        \n",
        "        #save labels\n",
        "        self.LABELS = kmeans.labels_\n",
        "        \n",
        "        #returning after converting to integer from float\n",
        "        return self.COLORS.astype(int)\n",
        "\n",
        "#import glob\n",
        "#path=''\n",
        "#img = glob.glob(path+'*.jp#g')\n",
        "#clusters = 1\n",
        "#img='blue (1).jpeg'\n",
        "#dc = DominantColors(img, clusters) \n",
        "#colors = dc.dominantColors()  \n",
        "#print(dist(colors))\n",
        "#print(colors)   \n",
        "acc()\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Colab Notebooks/test_col/green/12.jpg\n",
            "drive/My Drive/Colab Notebooks/test_col/green/images98.jpg\n",
            "drive/My Drive/Colab Notebooks/test_col/green/ROBOT (158).jpg\n",
            "drive/My Drive/Colab Notebooks/test_col/green/ruin (227).jpg\n",
            "83.65384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_TxgeHMHbvf",
        "colab_type": "text"
      },
      "source": [
        "TEXTURE(COLOUR) ACCURACY  =  83.65%\n",
        "\n",
        "OBJECT ACCURACY  =  75.12%"
      ]
    }
  ]
}